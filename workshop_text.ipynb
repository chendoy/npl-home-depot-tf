{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "job_cancel_str=\"scancel \" + os.environ['SLURM_JOBID']\n",
    "os.system(job_cancel_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Deep Learning Workshop 2021\n",
    "## Assignment 3 - Word Embeddings\n",
    " \n",
    "### Authors:\n",
    " \n",
    "1.   Chen Doytshman 205644941\n",
    "2.   Naor Kolet 205533060\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Concatenate\n",
    "from tensorflow.keras.layers import Dropout, Dense, Lambda, Multiply, Subtract, Flatten\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Activation, Reshape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Plots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Misc.\n",
    "import os\n",
    "import joblib\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "SEED = 42\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "DATA_DIR = 'home-depot-product-search-relevance'\n",
    "\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.endswith('zip')]\n",
    "for file in tqdm(files):\n",
    "    with zipfile.ZipFile(f'{DATA_DIR}/{file}') as zf:\n",
    "        zf.extractall(DATA_DIR)\n",
    "        os.remove(f'{DATA_DIR}/{file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.1 Data Preparation, EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'{DATA_DIR}/train.csv', index_col='id', encoding='latin-1')\n",
    "product_desc_df = pd.read_csv(f'{DATA_DIR}/product_descriptions.csv', index_col='product_uid')\n",
    "attributes_df = pd.read_csv(f'{DATA_DIR}/attributes.csv', index_col=['product_uid'], dtype={'product_uid': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f'{DATA_DIR}/test_labels.csv'\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    test_df = pd.read_csv(f'{DATA_DIR}/test.csv', index_col='id', encoding='latin-1')\n",
    "    sol = pd.read_csv(f'{DATA_DIR}/solution.csv', index_col='id', encoding='latin-1')\n",
    "    test_df = test_df[sol.relevance != -1]\n",
    "    test_df = test_df.join(sol.relevance, on='id')\n",
    "    test_df.to_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.countplot(data=train_df, x=\"relevance\")\n",
    "\n",
    "relevance_values = train_df[\"relevance\"].value_counts().sort_index()\n",
    "\n",
    "for p, label in zip(ax.patches, relevance_values):\n",
    "    ax.annotate(label, (p.get_x(), p.get_height()+0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_desc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_desc_df.iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_df[attributes_df.index.isin([100001])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def concat_attr(record):\n",
    "    name, value = record\n",
    "    name = f'{name} ' if not name.startswith('Bullet') else '' # Delete name if it's a \"Bullet..\"\n",
    "    return f'{name}{value} ;'\n",
    "\n",
    "product_groups = attributes_df.groupby('product_uid')\n",
    "for product_uid, df in product_groups:\n",
    "    joined_attrs = ' '.join(map(concat_attr, df.values))\n",
    "    product_desc_df.loc[product_uid, 'concat_desc'] = joined_attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "product_desc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain the connection between description and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_serie(serie):\n",
    "    serie = pd.Series(map(lambda x: list(map(ord, list(x))), serie.values), index=serie.index)\n",
    "    max_len = min(serie.apply(len).max(), 1500)\n",
    "    return pad_sequences(serie, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_desc(df):\n",
    "    df = df.join(product_desc_df['product_description'], on='product_uid')\n",
    "    df['product_description'] = df['product_title'] + ' : ' + df['product_description']\n",
    "    df = df[['product_uid', 'search_term', 'product_description', 'relevance']]\n",
    "    return df\n",
    "\n",
    "train_df = join_desc(train_df)\n",
    "test_df = join_desc(test_df)\n",
    "\n",
    "def split_x_y(df):\n",
    "    search_term = tokenize_serie(df['search_term'])\n",
    "    description = tokenize_serie(df['product_description'])\n",
    "    \n",
    "    search_term, description = map(lambda x: np.expand_dims(x,axis=2), [search_term, description])\n",
    "    return (search_term, description), df['relevance'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Character level LSTM\n",
    "\n",
    "## Using character level processing to predict search relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: explain the replacement of items' description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/prabhnoor0212/Siamese-Network-Text-Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.py_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = 0\n",
    "\n",
    "def common_model(length):\n",
    "    global sm\n",
    "    inp = Input(shape=(length, 1))\n",
    "    X = Conv1D(64, 10, activation='relu')(inp)\n",
    "    X = MaxPooling1D()(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Dense(128, activation='relu')(X)\n",
    "    \n",
    "    sm += 1\n",
    "    return Model(inp, X, name=f'siamese_model_{sm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_model(128).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_siamese_model(search_term_len, product_description_len, output_shape=1):\n",
    "    input_1 = Input(shape=(search_term_len, 1))\n",
    "    input_2 = Input(shape=(product_description_len, 1))\n",
    "    \n",
    "    lstm_1 = LSTM(128)(input_1)\n",
    "    lstm_2 = LSTM(128)(input_2)\n",
    "    \n",
    "    expand_layer = Lambda(lambda tensor: tensor[...,np.newaxis],name=\"expand_dim_layer\")\n",
    "    \n",
    "    expended_1 = expand_layer(lstm_1)\n",
    "    expended_2 = expand_layer(lstm_2)\n",
    "    \n",
    "    sm = common_model(128)\n",
    "\n",
    "    vector_1 = sm(expended_1)\n",
    "    \n",
    "    vector_2 = sm(expended_2)\n",
    "    \n",
    "    x3 = Subtract()([vector_1, vector_2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "\n",
    "    x1_ = Multiply()([vector_1, vector_1])\n",
    "    x2_ = Multiply()([vector_2, vector_2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    \n",
    "    x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])\n",
    "\n",
    "    conc = Concatenate(axis=-1)([x5,x4, x3])\n",
    "\n",
    "    x = Dense(100, activation=\"relu\")(conc)\n",
    "    x = Dropout(0.01)(x)\n",
    "    out = Dense(output_shape, activation=\"relu\", name = 'out')(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_siamese_model(train_st.shape[1],train_desc.shape[1]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name):\n",
    "    acc = 'val_loss'\n",
    "    acc_mode = 'min'\n",
    "#     acc = 'val_loss' if not model_name.startswith('ss') else 'val_loss'\n",
    "#     acc_mode = 'max' if not model_name.startswith('ss') else 'min'\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\n",
    "                              fr'./models/{model_name}.h5', \n",
    "                              monitor=acc, \n",
    "#                               verbose=1, \n",
    "                              save_best_only=True, \n",
    "                              mode=acc_mode)\n",
    "    earlystop = EarlyStopping(monitor=acc, mode=acc_mode, verbose=1, patience=4)\n",
    "    reduceLR = ReduceLROnPlateau(monitor = 'val_loss', mode = 'min', patience = 3,\n",
    "                            factor = 0.5, min_lr = 1e-6, verbose = 1)\n",
    "\n",
    "    return [checkpoint, reduceLR, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_gen, train_data, batch_size=128, use_saved=False):\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    model_name = model_gen.__name__[5:]\n",
    "        \n",
    "    if use_saved:\n",
    "        history = joblib.load(fr'./models/{model_name}_history.sav')\n",
    "    else:\n",
    "        callbacks = get_callbacks(model_name)\n",
    "        \n",
    "        (train_st, train_desc), train_rel = train_data\n",
    "        model = model_gen(train_st.shape[1],train_desc.shape[1])\n",
    "        history = model.fit(\n",
    "                            x=[train_st, train_desc],\n",
    "                            y=train_rel,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=20,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=callbacks\n",
    "                            )\n",
    "        \n",
    "        history = history.history\n",
    "        joblib.dump(history, fr'./models/{model_name}_history.sav')\n",
    "    \n",
    "    model = load_model(fr'./models/{model_name}.h5')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = split_x_y(train_df)\n",
    "char_model, _ = train_model(init_siamese_model, train_data, use_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_st, test_desc), test_rel = split_x_y(test_df)\n",
    "mse = char_model.evaluate([test_st, test_desc], test_rel)\n",
    "print(f'MSE loss on test set: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Naïve model-based benchmark with ```CountVectorizer```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: MARKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_series = train_df['product_description']\n",
    "search_term_series = train_df['search_term']\n",
    "desc_st = pd.concat([desc_series, search_term_series])\n",
    "corpus = desc_st.values\n",
    "vectorizer = CountVectorizer(analyzer='char')\n",
    "vectorizer.fit(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_desc = vectorizer.transform(desc_series).toarray()\n",
    "counts_st = vectorizer.transform(search_term_series).toarray()\n",
    "assert counts_desc.shape == counts_st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_naive(df):\n",
    "    desc_series = df['product_description']\n",
    "    search_term_series = df['search_term']\n",
    "    counts_desc = vectorizer.transform(desc_series).toarray()\n",
    "    counts_st = vectorizer.transform(search_term_series).toarray()\n",
    "    X = np.concatenate([counts_desc, counts_st], axis=1)\n",
    "    Y = df['relevance'].values\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = prepare_data_naive(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_saved = False\n",
    "rfr_path = 'models/rfr.sav'\n",
    "\n",
    "if use_saved:\n",
    "    rfr = joblib.load(rfr_path)\n",
    "else:\n",
    "    rfr = RandomForestRegressor(random_state=SEED, verbose=0, n_jobs=-1)\n",
    "    rfr.fit(train_x, train_y)\n",
    "    joblib.dump(rfr, rfr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = rfr.score(train_x, train_y)\n",
    "print(f'Got score of {train_score:.4f} according to the random forest score function on the train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred = rfr.predict(train_x)\n",
    "train_mse = mean_squared_error(train_y, train_y_pred)\n",
    "print(f'Got MSE of {train_mse:.4f} on the train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = prepare_data_naive(test_df)\n",
    "test_y_pred = rfr.predict(test_x)\n",
    "test_mse = mean_squared_error(test_y, test_y_pred)\n",
    "print(f'Got MSE of {test_mse:.4f} on the test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Using our model as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_model, char_history = char_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_char_model = Model(char_model.input, char_model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, _ = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_fechar = fe_char_model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_fechar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_saved = True\n",
    "\n",
    "fe_rfr_path = 'models/fe_char_rfr.sav'\n",
    "fe_xgb_path = 'models/fe_char_xgb.sav'\n",
    "\n",
    "if use_saved:\n",
    "    rfr_model = joblib.load(fe_rfr_path)\n",
    "    xgb_model = joblib.load(fe_xgb_path)\n",
    "else:\n",
    "    xgb_model = XGBRegressor(use_label_encoder=False, n_jobs=-1)\n",
    "    rfr_model = RandomForestRegressor(random_state=SEED, verbose=0, n_jobs=-1)\n",
    "\n",
    "    print('training xgb')\n",
    "    xgb_model.fit(train_preds_fechar, train_y)\n",
    "    print('training rfr')\n",
    "    rfr_model.fit(train_preds_fechar, train_y)\n",
    "\n",
    "    joblib.dump(rfr_model, fe_rfr_path)\n",
    "    joblib.dump(xgb_model, fe_xgb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = fe_char_model.predict([test_st, test_desc])\n",
    "test_y_pred = rfr_model.predict(test_features)\n",
    "test_mse = mean_squared_error(test_y, test_y_pred)\n",
    "print(f'Got MSE of {test_mse:.4f} on the test using RandomForest with feature extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = xgb_model.predict(test_features)\n",
    "test_mse = mean_squared_error(test_y, test_y_pred)\n",
    "print(f'Got MSE of {test_mse:.4f} on the test using XGBoost with feature extraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word embeddings and word level LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Preprocess the data to create tokens of words/character-combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Not only do angles make joints stronger, they also provide more consistent, straight corners. Simpson Strong-Tie offers a wide variety of angles in various sizes and thicknesses to handle light-duty jobs or projects where a structural connection is needed. Some can be bent (skewed) to match the project. For outdoor projects or those where moisture is present, use our ZMAX zinc-coated connectors, which provide extra resistance against corrosion (look for a \"Z\" at the end of the model number).Versatile connector for various 90 connections and home repair projectsStronger than angled nailing or screw fastening aloneHelp ensure joints are consistently straight and strongDimensions: 3 in. x 3 in. x 1-1/2 in.Made from 12-Gauge steelGalvanized for extra corrosion resistanceInstall with 10d common nails or #9 x 1-1/2 in. Strong-Drive SD screws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# TODO: seperate also on nubmer after chars, i.e. word100%\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z,(,)])\\.?\\s?(?=[A-Z])|(?<=[A-Z,(,)])\\.?\\s?(?=[A-Z][a-z])|$)', identifier)\n",
    "    return ' '.join([m.group(0).strip() for m in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'BEHR Premium Textured DeckOver 1-gal. #SC-141 Tugboat Wood and Concrete Coating : BEHR Premium Textured DECKOVER is an innovative solid color coating. It will bring your old, weathered wood or concrete back to life. The advanced 100% acrylic resin formula creates a durable coating for your tired and worn out deck, rejuvenating to a whole new look.  For the best results, be sure to properly prepare the surface using other applicable BEHR products displayed above.California residents: see&nbsp;Proposition 65 informationRevives wood and composite decks, railings, porches and boat docks, also great for concrete pool decks, patios and sidewalks100% acrylic solid color coatingResists cracking and peeling and conceals splinters and cracks up to 1/4 in.Provides a durable, mildew resistant finishCovers up to 75 sq. ft. in 2 coats per gallonCreates a textured, slip-resistant finishFor best results, prepare with the appropriate BEHR product for your wood or concrete surfaceActual paint colors may vary from on-screen and printer representationsColors available to be tinted in most storesOnline Price includes Paint Care fee in the following states: CA, CO, CT, ME, MN, OR, RI, VT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['product_description'] = train_df['product_description'].apply(camel_case_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_series = train_df['product_description']\n",
    "search_term_series = train_df['search_term']\n",
    "desc_st = pd.concat([desc_series, search_term_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(serie):\n",
    "    serie = pd.Series(tokenizer.texts_to_sequences(serie.values), index=serie.index)\n",
    "    max_len = min(serie.apply(len).max(), 1500)\n",
    "    return pad_sequences(serie, max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(desc_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_padded = tokenize_words(train_df['product_description'])\n",
    "st_padded = tokenize_words(train_df['search_term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "EMBEDDING_FILE = './GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n",
    "embed_size = 300\n",
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index) + 1\n",
    "\n",
    "nb_words = min(len(word_index), len(word_index))\n",
    "embedding_matrix = (np.random.rand(nb_words+1, embed_size) - 0.5) / 5.0\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    if word in embeddings_index:\n",
    "        embedding_vector = embeddings_index.get_vector(word)\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Constructing a Siamese network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_siamese_model_words(search_term_len, product_description_len, output_shape=1):\n",
    "    input_1 = Input(shape=(search_term_len,))\n",
    "    input_2 = Input(shape=(product_description_len,))\n",
    "    \n",
    "    embd1 = Embedding(max_features, \n",
    "                      embed_size, \n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=search_term_len,\n",
    "                      name='search_embd')(input_1)\n",
    "    \n",
    "    embd2 = Embedding(max_features,\n",
    "                      embed_size,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=product_description_len,\n",
    "                      name='desc_embd')(input_2)\n",
    "    \n",
    "    lstm_1 = LSTM(128)(embd1)\n",
    "    lstm_2 = LSTM(128)(embd2)\n",
    "    \n",
    "    expand_layer = Lambda(lambda tensor: tensor[...,np.newaxis],name=\"expand_dim_layer\")\n",
    "    \n",
    "    expended_1 = expand_layer(lstm_1)\n",
    "    expended_2 = expand_layer(lstm_2)\n",
    "    \n",
    "    sm = common_model(128)\n",
    "\n",
    "    vector_1 = sm(expended_1)\n",
    "    \n",
    "    vector_2 = sm(expended_2)\n",
    "    \n",
    "    x3 = Subtract()([vector_1, vector_2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "\n",
    "    x1_ = Multiply()([vector_1, vector_1])\n",
    "    x2_ = Multiply()([vector_2, vector_2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "    \n",
    "    x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([vector_1, vector_2])\n",
    "\n",
    "    conc = Concatenate(axis=-1)([x5,x4, x3])\n",
    "\n",
    "    x = Dense(100, activation=\"relu\")(conc)\n",
    "    x = Dropout(0.01)(x)\n",
    "    out = Dense(output_shape, activation=\"relu\", name = 'out')(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "    \n",
    "    model.get_layer('search_embd').trainable = False\n",
    "    model.get_layer('desc_embd').trainable = False\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_76 (InputLayer)           [(None, 17)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_77 (InputLayer)           [(None, 1051)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "search_embd (Embedding)         (None, 17, 300)      19937400    input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "desc_embd (Embedding)           (None, 1051, 300)    19937400    input_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                  (None, 128)          219648      search_embd[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                  (None, 128)          219648      desc_embd[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "expand_dim_layer (Lambda)       (None, 128, 1)       0           lstm_45[0][0]                    \n",
      "                                                                 lstm_46[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "siamese_model_18 (Model)        (None, 128)          484160      expand_dim_layer[0][0]           \n",
      "                                                                 expand_dim_layer[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_55 (Multiply)          (None, 128)          0           siamese_model_18[1][0]           \n",
      "                                                                 siamese_model_18[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "multiply_56 (Multiply)          (None, 128)          0           siamese_model_18[2][0]           \n",
      "                                                                 siamese_model_18[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "subtract_36 (Subtract)          (None, 128)          0           siamese_model_18[1][0]           \n",
      "                                                                 siamese_model_18[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 1)            0           siamese_model_18[1][0]           \n",
      "                                                                 siamese_model_18[2][0]           \n",
      "__________________________________________________________________________________________________\n",
      "subtract_37 (Subtract)          (None, 128)          0           multiply_55[0][0]                \n",
      "                                                                 multiply_56[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_54 (Multiply)          (None, 128)          0           subtract_36[0][0]                \n",
      "                                                                 subtract_36[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 257)          0           lambda_18[0][0]                  \n",
      "                                                                 subtract_37[0][0]                \n",
      "                                                                 multiply_54[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 100)          25800       concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 100)          0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 1)            101         dropout_18[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 40,824,157\n",
      "Trainable params: 949,357\n",
      "Non-trainable params: 39,874,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "init_siamese_model_words(st_padded.shape[1],desc_padded.shape[1]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "463/463 [==============================] - 52s 112ms/step - loss: 0.4013 - val_loss: 0.2893 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "463/463 [==============================] - 50s 108ms/step - loss: 0.2811 - val_loss: 0.3184 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "463/463 [==============================] - 50s 109ms/step - loss: 0.2767 - val_loss: 0.2974 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "463/463 [==============================] - 52s 113ms/step - loss: 0.2720 - val_loss: 0.2740 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "463/463 [==============================] - 53s 113ms/step - loss: 0.2630 - val_loss: 0.2738 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "463/463 [==============================] - 51s 109ms/step - loss: 0.2561 - val_loss: 0.3268 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "463/463 [==============================] - 51s 110ms/step - loss: 0.2498 - val_loss: 0.2757 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "463/463 [==============================] - ETA: 0s - loss: 0.2455\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "463/463 [==============================] - 51s 110ms/step - loss: 0.2455 - val_loss: 0.2766 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "463/463 [==============================] - 53s 114ms/step - loss: 0.2301 - val_loss: 0.2580 - lr: 5.0000e-04\n",
      "Epoch 10/20\n",
      "463/463 [==============================] - 50s 108ms/step - loss: 0.2250 - val_loss: 0.2743 - lr: 5.0000e-04\n",
      "Epoch 11/20\n",
      "463/463 [==============================] - 51s 109ms/step - loss: 0.2223 - val_loss: 0.2661 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "463/463 [==============================] - ETA: 0s - loss: 0.2178\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "463/463 [==============================] - 51s 110ms/step - loss: 0.2178 - val_loss: 0.2637 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "463/463 [==============================] - 51s 110ms/step - loss: 0.2104 - val_loss: 0.2694 - lr: 2.5000e-04\n",
      "Epoch 00013: early stopping\n"
     ]
    }
   ],
   "source": [
    "train_data = ((st_padded, desc_padded), train_df['relevance'].values)\n",
    "word_model, _ = train_model(init_siamese_model_words, train_data, use_saved=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc_padded = tokenize_words(test_df['product_description'])\n",
    "test_st_padded = tokenize_words(test_df['search_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112067, 17), (112067, 1051))"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_st_padded.shape, test_desc_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3503/3503 [==============================] - 91s 26ms/step - loss: 0.2881\n",
      "MSE loss on test set: 0.2881\n"
     ]
    }
   ],
   "source": [
    "mse = word_model.evaluate([test_st_padded, test_desc_padded], test_rel)\n",
    "print(f'MSE loss on test set: {mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Using our model as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_word_model = Model(word_model.input, word_model.layers[-3].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, _ = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_feword = fe_word_model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74067, 100)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds_feword.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training xgb\n",
      "training rfr\n"
     ]
    }
   ],
   "source": [
    "use_saved = False\n",
    "\n",
    "fe_rfr_path = 'models/fe_word_rfr.sav'\n",
    "fe_xgb_path = 'models/fe_word_xgb.sav'\n",
    "\n",
    "if use_saved:\n",
    "    rfr_model = joblib.load(fe_rfr_path)\n",
    "    xgb_model = joblib.load(fe_xgb_path)\n",
    "else:\n",
    "    xgb_model = XGBRegressor(use_label_encoder=False, n_jobs=-1)\n",
    "    rfr_model = RandomForestRegressor(random_state=SEED, verbose=0, n_jobs=-1)\n",
    "\n",
    "    print('training xgb')\n",
    "    xgb_model.fit(train_preds_feword, train_y)\n",
    "    print('training rfr')\n",
    "    rfr_model.fit(train_preds_feword, train_y)\n",
    "\n",
    "    joblib.dump(rfr_model, fe_rfr_path)\n",
    "    joblib.dump(xgb_model, fe_xgb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MSE of 0.2873 on the test using RandomForest with feature extraction\n"
     ]
    }
   ],
   "source": [
    "test_features = fe_word_model.predict([test_desc_padded, test_st_padded])\n",
    "test_y_pred = rfr_model.predict(test_features)\n",
    "test_mse = mean_squared_error(test_y, test_y_pred)\n",
    "print(f'Got MSE of {test_mse:.4f} on the test using RandomForest with feature extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got MSE of 0.2882 on the test using XGBoost with feature extraction\n"
     ]
    }
   ],
   "source": [
    "test_y_pred = xgb_model.predict(test_features)\n",
    "test_mse = mean_squared_error(test_y, test_y_pred)\n",
    "print(f'Got MSE of {test_mse:.4f} on the test using XGBoost with feature extraction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparison of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
